{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyH_-cJvPjzy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sequence_length = 50\n",
        "patch_size = 5\n",
        "DG = 16\n",
        "DL = 32\n",
        "vocab_size = 256\n",
        "\n",
        "class ByteEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(ByteEmbedding, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x) + self.pos_embed[:, :x.size(1), :]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = torch.randint(0, vocab_size, (1, sequence_length))\n",
        "byte_embedding = ByteEmbedding(vocab_size, DG)\n",
        "embedded_sequence = byte_embedding(input_sequence)"
      ],
      "metadata": {
        "id": "F1_gKwXSPy7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzvaO7kixVec",
        "outputId": "7fe5c500-912d-4063-e04e-661503c0f8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 55,   4,  96,  20,  50,   8,  11, 244,  55, 213, 231, 181, 172, 126,\n",
              "          89,  49, 170,  78, 119, 214, 224, 218,  65, 197, 212, 134, 200,  90,\n",
              "         217, 174, 244,  90, 189,   3, 162, 183, 165, 207,  55, 195,  66,  95,\n",
              "          36,  95,  44, 105, 220,  75, 148, 195]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence.shape, embedded_sequence.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJPM0-0sPy-D",
        "outputId": "8160bd7d-02b1-4a43-aa81-5eff279ec7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 50]), torch.Size([1, 50, 16]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        assert seq_len % self.patch_size == 0, \"Sequence length must be divisible by patch size\"\n",
        "\n",
        "        num_patches = seq_len // self.patch_size\n",
        "        x = x.view(batch_size, num_patches, self.patch_size * embed_dim)\n",
        "\n",
        "        pad_embed = nn.Parameter(torch.zeros(batch_size, 1, self.patch_size * embed_dim))\n",
        "        x = torch.cat((pad_embed, x), dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9xOY-fZgPzMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embedding = PatchEmbedding(patch_size, DG)\n",
        "patches = patch_embedding(embedded_sequence)  # (1, 11, 80)"
      ],
      "metadata": {
        "id": "B-Xb4-0TPzTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_layers):\n",
        "        super(GlobalTransformer, self).__init__()\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=patch_size*DG, nhead=nhead, dim_feedforward=256, dropout=0.1)\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.transpose(0, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Mw8ijPYBWMBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_transformer = GlobalTransformer(patch_size * DG, nhead=4, num_layers=2)\n",
        "global_output = global_transformer(patches)  # (1, 11, 80)"
      ],
      "metadata": {
        "id": "S39BNmuHWWo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR3HzvwmW1iT",
        "outputId": "2163e337-8431-47e6-bf27-a96a2861206d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 11, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_layers):\n",
        "        super(LocalTransformer, self).__init__()\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=128, dropout=0.1)\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.transpose(0, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JS6lL28FW1lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_transformer = LocalTransformer(DL, nhead=4, num_layers=2)"
      ],
      "metadata": {
        "id": "CHkxXIsyW1os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_to_local_proj = nn.Linear(patch_size * DG, DL)\n",
        "projected_global_output = global_to_local_proj(global_output)  # (1, 11, 32)"
      ],
      "metadata": {
        "id": "NZbcbCRRZfbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projected_global_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcV1fy9gygm5",
        "outputId": "5fd28c65-e543-4ce0-c7b8-3a6a38cced34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 11, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_outputs = []\n",
        "for i in range(1, projected_global_output.size(1)):\n",
        "    patch_input = projected_global_output[:, i, :].unsqueeze(1).repeat(1, patch_size, 1)  # (batch_size, patch_size, DL)\n",
        "    local_output = local_transformer(patch_input)\n",
        "    local_outputs.append(local_output)"
      ],
      "metadata": {
        "id": "grY99waxZ143"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_outputs = torch.cat(local_outputs, dim=1)  # (1, 10, DL)\n",
        "flattened_local_outputs = local_outputs.view(-1, DL)  # (10 * 5, DL)\n",
        "\n",
        "final_proj = nn.Linear(DL, vocab_size)\n",
        "logits = final_proj(flattened_local_outputs)  # (50, vocab_size)\n",
        "\n",
        "probabilities = F.softmax(logits, dim=-1)  # (50, vocab_size)"
      ],
      "metadata": {
        "id": "ZJ9mB-O5XBq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_outputs.shape, flattened_local_outputs.shape, logits.shape, probabilities.shape, final_proj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akLgaF4Id752",
        "outputId": "57e951d2-9e15-4d6c-fac7-1fc0eaf8d81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 50, 32]),\n",
              " torch.Size([50, 32]),\n",
              " torch.Size([50, 256]),\n",
              " torch.Size([50, 256]),\n",
              " Linear(in_features=32, out_features=256, bias=True))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[I am a] [boy that ]"
      ],
      "metadata": {
        "id": "yEr85R9ppuki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalPatchEncoder(nn.Module): # Masked Con\n",
        "    def __init__(self, embed_dim, num_layers, kernel_size, patch_size):\n",
        "        super(ConvolutionalPatchEncoder, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(embed_dim, embed_dim, kernel_size, padding=kernel_size//2) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        for conv in self.convs:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        num_patches = seq_len // self.patch_size\n",
        "        x = x.reshape(batch_size, num_patches, self.patch_size * embed_dim)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KvL4iK6-pun4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_patch_encoder = ConvolutionalPatchEncoder(embed_dim=DG, num_layers=2, kernel_size=3, patch_size=5)\n",
        "embedded_sequence = byte_embedding(input_sequence)\n",
        "conv_patches = conv_patch_encoder(embedded_sequence)"
      ],
      "metadata": {
        "id": "_tpLsosupxC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_patches.shape"
      ],
      "metadata": {
        "id": "drEgzFcLqI4v",
        "outputId": "2ab7d246-ae58-4487-d9a4-148d23fa4eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cEwKPdP8GUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = embedded_sequence\n",
        "x = x.transpose(1, 2)\n",
        "for conv in convs:\n",
        "  x = F.relu(conv(x))"
      ],
      "metadata": {
        "id": "ecQkKMEd8vmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsoKVmLq9K1F",
        "outputId": "09957f30-8df8-4669-f101-9ab231d23a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 16, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jApbnrTU-FQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Conv1d(16, 33, 3, padding=1)\n",
        "input = torch.randn(20, 16, 50)\n",
        "output = m(input)"
      ],
      "metadata": {
        "id": "rY0EXH1a-FT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6x3eYnV-KES",
        "outputId": "a518705a-0cde-4b51-c2f5-ae612a5fcae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 33, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}